# Prediction function - only defined in inference mode
if not TRAIN:
    def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:
        df_seq = sequence.to_pandas()
        
        # Apply EXACT same feature engineering as training
        linear_accel = remove_gravity_from_acc(df_seq, df_seq)
        df_seq['linear_acc_x'] = linear_accel[:, 0]
        df_seq['linear_acc_y'] = linear_accel[:, 1] 
        df_seq['linear_acc_z'] = linear_accel[:, 2]
        df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)
        df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)
        
        angular_vel = calculate_angular_velocity_from_quat(df_seq)
        df_seq['angular_vel_x'] = angular_vel[:, 0]
        df_seq['angular_vel_y'] = angular_vel[:, 1] 
        df_seq['angular_vel_z'] = angular_vel[:, 2]
        df_seq['angular_distance'] = calculate_angular_distance(df_seq)
        
        # Process ToF sensors - statistical features only (consistent with training)
        for i in range(1, 6):
            pixel_cols = [f"tof_{i}_v{p}" for p in range(64)]
            if all(col in df_seq.columns for col in pixel_cols):
                tof_data = df_seq[pixel_cols].replace(-1, np.nan)
                df_seq[f'tof_{i}_mean'] = tof_data.mean(axis=1)
                df_seq[f'tof_{i}_std'] = tof_data.std(axis=1)
                df_seq[f'tof_{i}_min'] = tof_data.min(axis=1)
                df_seq[f'tof_{i}_max'] = tof_data.max(axis=1)
            else:
                # Handle missing ToF data
                df_seq[f'tof_{i}_mean'] = 0.0
                df_seq[f'tof_{i}_std'] = 0.0
                df_seq[f'tof_{i}_min'] = 0.0
                df_seq[f'tof_{i}_max'] = 0.0
        
        # Use ONLY the saved feature columns (no raw pixel data)
        mat_unscaled = df_seq[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')
        mat_scaled = scaler.transform(mat_unscaled)
        pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')
        
        # Debug print for troubleshooting
        if pad_input.shape[2] != len(final_feature_cols):
            print(f"ERROR: Feature mismatch! Expected {len(final_feature_cols)}, got {pad_input.shape[2]}")
            print(f"final_feature_cols: {len(final_feature_cols)}")
            print(f"pad_input.shape: {pad_input.shape}")
        
        all_preds = [model.predict(pad_input, verbose=0)[0] for model in models]
        avg_pred = np.mean(all_preds, axis=0)
        return str(gesture_classes[avg_pred.argmax()])
    
    print("  Prediction function defined and ready for inference")
    print(f"  Expected feature count: {len(final_feature_cols)}")
    print(f"  Feature list: {final_feature_cols[:10]}... (showing first 10)")
else:
    print("  Prediction function skipped in training mode"){
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd54d847",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:28.364685Z",
     "iopub.status.busy": "2025-07-25T13:57:28.364395Z",
     "iopub.status.idle": "2025-07-25T13:57:43.790879Z",
     "shell.execute_reply": "2025-07-25T13:57:43.790129Z"
    },
    "papermill": {
     "duration": 15.431719,
     "end_time": "2025-07-25T13:57:43.792422",
     "exception": false,
     "start_time": "2025-07-25T13:57:28.360703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 13:57:32.378952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753451852.561385      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753451852.623912      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from scipy.spatial.transform import Rotation as R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bb8366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:43.798575Z",
     "iopub.status.busy": "2025-07-25T13:57:43.797882Z",
     "iopub.status.idle": "2025-07-25T13:57:43.802619Z",
     "shell.execute_reply": "2025-07-25T13:57:43.801907Z"
    },
    "papermill": {
     "duration": 0.008786,
     "end_time": "2025-07-25T13:57:43.803852",
     "exception": false,
     "start_time": "2025-07-25T13:57:43.795066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258ab52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:43.809464Z",
     "iopub.status.busy": "2025-07-25T13:57:43.808840Z",
     "iopub.status.idle": "2025-07-25T13:57:43.813944Z",
     "shell.execute_reply": "2025-07-25T13:57:43.813258Z"
    },
    "papermill": {
     "duration": 0.009069,
     "end_time": "2025-07-25T13:57:43.815054",
     "exception": false,
     "start_time": "2025-07-25T13:57:43.805985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · tensorflow 2.18.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN = False                     # ← set to True when you want to train\n",
    "RAW_DIR = Path(\"data\")\n",
    "PRETRAINED_DIR = Path(\"OG-model\")\n",
    "EXPORT_DIR = Path(\"new-model\")                                    # artefacts will be saved here\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 95\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4\n",
    "EPOCHS = 160\n",
    "PATIENCE = 40\n",
    "\n",
    "\n",
    "print(\"▶ imports ready · tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd1e712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:43.820256Z",
     "iopub.status.busy": "2025-07-25T13:57:43.820062Z",
     "iopub.status.idle": "2025-07-25T13:57:43.827294Z",
     "shell.execute_reply": "2025-07-25T13:57:43.826779Z"
    },
    "papermill": {
     "duration": 0.011039,
     "end_time": "2025-07-25T13:57:43.828394",
     "exception": false,
     "start_time": "2025-07-25T13:57:43.817355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tensor Manipulations\n",
    "def time_sum(x):\n",
    "    return K.sum(x, axis=1)\n",
    "\n",
    "def squeeze_last_axis(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def expand_last_axis(x):\n",
    "    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "# Residual CNN Block with SE\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n",
    "                          kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5989d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:43.833739Z",
     "iopub.status.busy": "2025-07-25T13:57:43.833516Z",
     "iopub.status.idle": "2025-07-25T13:57:43.839324Z",
     "shell.execute_reply": "2025-07-25T13:57:43.838825Z"
    },
    "papermill": {
     "duration": 0.009733,
     "end_time": "2025-07-25T13:57:43.840323",
     "exception": false,
     "start_time": "2025-07-25T13:57:43.830590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalizes and cleans the time series sequence. \n",
    "\n",
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "\n",
    "# MixUp the data argumentation in order to regularize the neural network. \n",
    "\n",
    "class MixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, alpha=0.2):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.indices = np.arange(len(X))\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx], self.y[idx]\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        perm = np.random.permutation(len(Xb))\n",
    "        X_mix = lam * Xb + (1-lam) * Xb[perm]\n",
    "        y_mix = lam * yb + (1-lam) * yb[perm]\n",
    "        return X_mix, y_mix\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e275d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:43.845632Z",
     "iopub.status.busy": "2025-07-25T13:57:43.845429Z",
     "iopub.status.idle": "2025-07-25T13:57:43.853122Z",
     "shell.execute_reply": "2025-07-25T13:57:43.852601Z"
    },
    "papermill": {
     "duration": 0.011554,
     "end_time": "2025-07-25T13:57:43.854091",
     "exception": false,
     "start_time": "2025-07-25T13:57:43.842537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    \n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "\n",
    "            # Calculate the relative rotation\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            \n",
    "            # Convert delta rotation to angular velocity vector\n",
    "            # The rotation vector (Euler axis * angle) scaled by 1/dt\n",
    "            # is a good approximation for small delta_rot\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            # If quaternion is invalid, angular velocity remains zero\n",
    "            pass\n",
    "            \n",
    "    return angular_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4f9f43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:43.859334Z",
     "iopub.status.busy": "2025-07-25T13:57:43.859127Z",
     "iopub.status.idle": "2025-07-25T13:57:43.864605Z",
     "shell.execute_reply": "2025-07-25T13:57:43.864093Z"
    },
    "papermill": {
     "duration": 0.009315,
     "end_time": "2025-07-25T13:57:43.865711",
     "exception": false,
     "start_time": "2025-07-25T13:57:43.856396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 # Или np.nan, в зависимости от желаемого поведения\n",
    "            continue\n",
    "        try:\n",
    "            # Преобразование кватернионов в объекты Rotation\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "\n",
    "            # Вычисление углового расстояния: 2 * arccos(|real(p * q*)|)\n",
    "            # где p* - сопряженный кватернион q\n",
    "            # В scipy.spatial.transform.Rotation, r1.inv() * r2 дает относительное вращение.\n",
    "            # Угол этого относительного вращения - это и есть угловое расстояние.\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            \n",
    "            # Угол rotation vector соответствует угловому расстоянию\n",
    "            # Норма rotation vector - это угол в радианах\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
    "            pass\n",
    "            \n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c07ca9d",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:43.871167Z",
     "iopub.status.busy": "2025-07-25T13:57:43.870954Z",
     "iopub.status.idle": "2025-07-25T13:57:46.165006Z",
     "shell.execute_reply": "2025-07-25T13:57:46.164201Z"
    },
    "papermill": {
     "duration": 2.298422,
     "end_time": "2025-07-25T13:57:46.166440",
     "exception": false,
     "start_time": "2025-07-25T13:57:43.868018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753451864.599578      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "def build_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    # IMU deep branch\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "\n",
    "    # TOF/Thermal lighter branch\n",
    "    x2 = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "    x2 = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "\n",
    "    merged = Concatenate()([x1, x2])\n",
    "\n",
    "    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xc = GaussianNoise(0.09)(merged)\n",
    "    xc = Dense(16, activation='elu')(xc)\n",
    "    \n",
    "    x = Concatenate()([xa, xb, xc])\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "\n",
    "    for units, drop in [(256, 0.5), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "tmp_model = build_two_branch_model(127,7,325,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55b0ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:46.172680Z",
     "iopub.status.busy": "2025-07-25T13:57:46.172422Z",
     "iopub.status.idle": "2025-07-25T13:57:56.735899Z",
     "shell.execute_reply": "2025-07-25T13:57:56.734973Z"
    },
    "papermill": {
     "duration": 10.567748,
     "end_time": "2025-07-25T13:57:56.737087",
     "exception": false,
     "start_time": "2025-07-25T13:57:46.169339",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ INFERENCE MODE – loading artefacts from /kaggle/input/cmi-d-111\n",
      "  Loading models for ensemble inference...\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_0.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_1.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_2.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_3.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_4.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_5.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_6.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_7.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_8.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_9.h5\n",
      "--------------------------------------------------\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_0.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_1.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_2.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_3.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_4.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_5.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_6.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_7.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_8.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_9.h5\n",
      "--------------------------------------------------\n",
      "[INFO]NumUseModels:20\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE MODE - Activates when TRAIN = False\n",
    "if not TRAIN:\n",
    "    print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "    final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "    pad_len        = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "    scaler         = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "\n",
    "    custom_objs = {\n",
    "        'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis,\n",
    "        'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer,\n",
    "    }\n",
    "\n",
    "    # ----------------------------------------------------------------- #\n",
    "    # Load any Models\n",
    "    # * is 2 Train Model Load\n",
    "    # ----------------------------------------------------------------- #\n",
    "\n",
    "    models = []\n",
    "    print(f\"  Loading models for ensemble inference...\")\n",
    "    for fold in range(10):\n",
    "        MODEL_DIR = \"new-model\"\n",
    "        \n",
    "        model_path = f\"{MODEL_DIR}/D-111_{fold}.h5\"\n",
    "        print(\">>>LoadModel>>>\",model_path)\n",
    "        model = load_model(model_path, compile=False, custom_objects=custom_objs)\n",
    "        models.append(model)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    for fold in range(10):\n",
    "        MODEL_DIR = \"new-model\"\n",
    "        \n",
    "        model_path = f\"{MODEL_DIR}/v0629_{fold}.h5\"\n",
    "        print(\">>>LoadModel>>>\",model_path)\n",
    "        model = load_model(model_path, compile=False, custom_objects=custom_objs)\n",
    "        models.append(model)\n",
    "    print(\"-\"*50)\n",
    "    print(f\"[INFO]NumUseModels:{len(models)}\")\n",
    "else:\n",
    "    print(\"▶ INFERENCE DISABLED – Set TRAIN=False to enable inference mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a89cac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:56.744415Z",
     "iopub.status.busy": "2025-07-25T13:57:56.744170Z",
     "iopub.status.idle": "2025-07-25T13:57:56.751472Z",
     "shell.execute_reply": "2025-07-25T13:57:56.750956Z"
    },
    "papermill": {
     "duration": 0.012012,
     "end_time": "2025-07-25T13:57:56.752508",
     "exception": false,
     "start_time": "2025-07-25T13:57:56.740496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction function - only defined in inference mode\n",
    "if not TRAIN:\n",
    "    def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "        df_seq = sequence.to_pandas()\n",
    "        linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "        df_seq['linear_acc_x'], df_seq['linear_acc_y'], df_seq['linear_acc_z'] = linear_accel[:, 0], linear_accel[:, 1], linear_accel[:, 2]\n",
    "        df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "        df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "        angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "        df_seq['angular_vel_x'], df_seq['angular_vel_y'], df_seq['angular_vel_z'] = angular_vel[:, 0], angular_vel[:, 1], angular_vel[:, 2]\n",
    "        df_seq['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "        \n",
    "        for i in range(1, 6):\n",
    "            pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]; tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "            df_seq[f'tof_{i}_mean'], df_seq[f'tof_{i}_std'], df_seq[f'tof_{i}_min'], df_seq[f'tof_{i}_max'] = tof_data.mean(axis=1), tof_data.std(axis=1), tof_data.min(axis=1), tof_data.max(axis=1)\n",
    "            \n",
    "        mat_unscaled = df_seq[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "        mat_scaled = scaler.transform(mat_unscaled)\n",
    "        pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "        \n",
    "        all_preds = [model.predict(pad_input, verbose=0)[0] for model in models] # 主出力のみ取得\n",
    "        avg_pred = np.mean(all_preds, axis=0)\n",
    "        return str(gesture_classes[avg_pred.argmax()])\n",
    "    \n",
    "    print(\"  Prediction function defined and ready for inference\")\n",
    "else:\n",
    "    print(\"  Prediction function skipped in training mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Pipeline - Activates when TRAIN = True\n",
    "if TRAIN:\n",
    "    print(\"▶ TRAINING MODE – loading and preprocessing data\")\n",
    "    \n",
    "    # Load training data\n",
    "    train_df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "    train_demographics = pd.read_csv(RAW_DIR / \"train_demographics.csv\")\n",
    "    \n",
    "    print(f\"  Loaded {len(train_df)} training sequences\")\n",
    "    print(f\"  Unique subjects: {train_df['subject'].nunique()}\")\n",
    "    print(f\"  Gesture classes: {sorted(train_df['gesture'].unique())}\")\n",
    "    \n",
    "    # Feature Engineering Function\n",
    "    def engineer_features(df_seq):\n",
    "        \"\"\"Apply the same feature engineering as in inference\"\"\"\n",
    "        # Physics-based feature engineering\n",
    "        linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "        df_seq['linear_acc_x'] = linear_accel[:, 0]\n",
    "        df_seq['linear_acc_y'] = linear_accel[:, 1] \n",
    "        df_seq['linear_acc_z'] = linear_accel[:, 2]\n",
    "        df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + \n",
    "                                          df_seq['linear_acc_y']**2 + \n",
    "                                          df_seq['linear_acc_z']**2)\n",
    "        df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "        \n",
    "        # Calculate angular features\n",
    "        angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "        df_seq['angular_vel_x'] = angular_vel[:, 0]\n",
    "        df_seq['angular_vel_y'] = angular_vel[:, 1]\n",
    "        df_seq['angular_vel_z'] = angular_vel[:, 2]\n",
    "        df_seq['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "        \n",
    "        # Process ToF sensor data (5 sensors with 64 pixels each)\n",
    "        for i in range(1, 6):\n",
    "            pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "            if all(col in df_seq.columns for col in pixel_cols):\n",
    "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "                df_seq[f'tof_{i}_mean'] = tof_data.mean(axis=1)\n",
    "                df_seq[f'tof_{i}_std'] = tof_data.std(axis=1)\n",
    "                df_seq[f'tof_{i}_min'] = tof_data.min(axis=1)\n",
    "                df_seq[f'tof_{i}_max'] = tof_data.max(axis=1)\n",
    "            else:\n",
    "                # Handle missing ToF data\n",
    "                for stat in ['mean', 'std', 'min', 'max']:\n",
    "                    df_seq[f'tof_{i}_{stat}'] = 0.0\n",
    "        \n",
    "        return df_seq\n",
    "    \n",
    "    # Process all sequences and apply feature engineering\n",
    "    print(\"  Applying feature engineering...\")\n",
    "    processed_sequences = []\n",
    "    labels = []\n",
    "    subjects = []\n",
    "    \n",
    "    for seq_id in train_df['sequence_id'].unique():\n",
    "        seq_data = train_df[train_df['sequence_id'] == seq_id].copy()\n",
    "        seq_data = engineer_features(seq_data)\n",
    "        \n",
    "        # Define feature columns (same structure as inference)\n",
    "        base_features = ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']\n",
    "        physics_features = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z', 'linear_acc_mag', \n",
    "                           'linear_acc_mag_jerk', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', \n",
    "                           'angular_distance']\n",
    "        thermal_features = [f'thm_{i}' for i in range(1, 6)]\n",
    "        tof_stat_features = [f'tof_{i}_{stat}' for i in range(1, 6) for stat in ['mean', 'std', 'min', 'max']]\n",
    "        \n",
    "        # Include raw ToF pixel data if available\n",
    "        tof_pixel_features = []\n",
    "        for i in range(1, 6):\n",
    "            pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "            if all(col in seq_data.columns for col in pixel_cols):\n",
    "                tof_pixel_features.extend(pixel_cols)\n",
    "        \n",
    "        feature_cols = base_features + physics_features + thermal_features + tof_stat_features + tof_pixel_features\n",
    "        \n",
    "        # Filter available features\n",
    "        available_features = [col for col in feature_cols if col in seq_data.columns]\n",
    "        \n",
    "        # Extract sequence matrix\n",
    "        seq_matrix = seq_data[available_features].ffill().bfill().fillna(0).values.astype('float32')\n",
    "        processed_sequences.append(seq_matrix)\n",
    "        labels.append(seq_data['gesture'].iloc[0])\n",
    "        subjects.append(seq_data['subject'].iloc[0])\n",
    "    \n",
    "    print(f\"  Processed {len(processed_sequences)} sequences\")\n",
    "    print(f\"  Feature dimensions: {processed_sequences[0].shape}\")\n",
    "    \n",
    "    # Determine sequence length using percentile\n",
    "    seq_lengths = [seq.shape[0] for seq in processed_sequences]\n",
    "    pad_len = int(np.percentile(seq_lengths, PAD_PERCENTILE))\n",
    "    print(f\"  Sequence padding length (95th percentile): {pad_len}\")\n",
    "    \n",
    "    # Pad sequences\n",
    "    X = pad_sequences(processed_sequences, maxlen=pad_len, padding='post', \n",
    "                     truncating='post', dtype='float32')\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(labels)\n",
    "    gesture_classes = label_encoder.classes_\n",
    "    y_categorical = to_categorical(y_encoded, num_classes=len(gesture_classes))\n",
    "    \n",
    "    print(f\"  Final data shape: {X.shape}\")\n",
    "    print(f\"  Number of classes: {len(gesture_classes)}\")\n",
    "    print(f\"  Gesture classes: {list(gesture_classes)}\")\n",
    "    \n",
    "    # Determine feature dimensions\n",
    "    n_features = X.shape[-1]\n",
    "    \n",
    "    # Estimate IMU vs ToF dimensions based on available features\n",
    "    imu_dim = len([col for col in available_features if any(x in col for x in ['acc_', 'rot_', 'linear_acc', 'angular_'])])\n",
    "    tof_dim = n_features - imu_dim\n",
    "    \n",
    "    print(f\"  IMU features: {imu_dim}, ToF/Thermal features: {tof_dim}\")\n",
    "    \n",
    "    # Fit scaler on training data\n",
    "    print(\"  Fitting data scaler...\")\n",
    "    X_flat = X.reshape(-1, X.shape[-1])\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_flat = scaler.fit_transform(X_flat)\n",
    "    X_scaled = X_scaled_flat.reshape(X.shape)\n",
    "    \n",
    "    # Setup cross-validation\n",
    "    subjects = np.array(subjects)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Create directories\n",
    "    EXPORT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_scaled, y_encoded, subjects)):\n",
    "        print(f\"\\n▶ Training Fold {fold+1}/10\")\n",
    "        \n",
    "        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "        y_train, y_val = y_categorical[train_idx], y_categorical[val_idx]\n",
    "        \n",
    "        print(f\"  Train: {len(X_train)} samples, Val: {len(X_val)} samples\")\n",
    "        \n",
    "        # Build model\n",
    "        model = build_two_branch_model(pad_len, imu_dim, tof_dim, len(gesture_classes), wd=WD)\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=LR_INIT, weight_decay=WD)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Create MixUp generator\n",
    "        mixup_gen = MixupGenerator(X_train, y_train, BATCH_SIZE, MIXUP_ALPHA)\n",
    "        \n",
    "        print(f\"  Training with MixUp (alpha={MIXUP_ALPHA})...\")\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            mixup_gen,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        fold_scores.append(val_acc)\n",
    "        \n",
    "        print(f\"  Fold {fold+1} - Val Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save model\n",
    "        model_path = EXPORT_DIR / f\"D-111_{fold}.h5\"\n",
    "        model.save(model_path)\n",
    "        print(f\"  Saved model: {model_path}\")\n",
    "    \n",
    "    # Print overall results\n",
    "    print(f\"\\n▶ Training completed!\")\n",
    "    print(f\"  Cross-validation scores: {[f'{score:.4f}' for score in fold_scores]}\")\n",
    "    print(f\"  Mean CV Score: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")\n",
    "    \n",
    "    # Save training artifacts\n",
    "    print(\"  Saving training artifacts...\")\n",
    "    \n",
    "    # Save feature columns\n",
    "    np.save(EXPORT_DIR / \"feature_cols.npy\", available_features)\n",
    "    \n",
    "    # Save sequence maxlen\n",
    "    np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n",
    "    \n",
    "    # Save scaler\n",
    "    joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n",
    "    \n",
    "    # Save gesture classes\n",
    "    np.save(EXPORT_DIR / \"gesture_classes.npy\", gesture_classes)\n",
    "    \n",
    "    print(f\"  All artifacts saved to {EXPORT_DIR}\")\n",
    "    \n",
    "else:\n",
    "    print(\"▶ TRAINING DISABLED – Set TRAIN=True to enable training mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06769b72",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-07-25T13:57:56.759741Z",
     "iopub.status.busy": "2025-07-25T13:57:56.759221Z",
     "iopub.status.idle": "2025-07-25T13:58:19.745997Z",
     "shell.execute_reply": "2025-07-25T13:58:19.745104Z"
    },
    "papermill": {
     "duration": 22.991611,
     "end_time": "2025-07-25T13:58:19.747201",
     "exception": false,
     "start_time": "2025-07-25T13:57:56.755590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 13:57:57.876666: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "I0000 00:00:1753451878.802419      59 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-07-25 13:58:03.429684: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-07-25 13:58:08.959976: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-07-25 13:58:14.555962: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-07-25 13:58:19.587773: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    }
   ],
   "source": [
    "# Kaggle Evaluation Server - only runs in inference mode\n",
    "if not TRAIN:\n",
    "    import sys\n",
    "    sys.path.append('data')  # Add data directory to path for kaggle_evaluation import\n",
    "    import kaggle_evaluation.cmi_inference_server\n",
    "    \n",
    "    print(\"  Setting up Kaggle evaluation server...\")\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "    \n",
    "    print(\"  Starting inference server...\")\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        inference_server.run_local_gateway(\n",
    "            data_paths=(\n",
    "                '/data/test.csv',\n",
    "                '/data/test_demographics.csv',\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    print(\"  Kaggle evaluation server skipped in training mode\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "isSourceIdPinned": false,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7941907,
     "sourceId": 12575412,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 58.188414,
   "end_time": "2025-07-25T13:58:22.568691",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-25T13:57:24.380277",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
